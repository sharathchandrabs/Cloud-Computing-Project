import sys
from pyspark import SparkContext
from numpy import *
from numpy.linalg import inv
import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

from pandas import DataFrame
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
from numpy import loadtxt, where

min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))

sc = SparkContext(appName="LogisticRegression")
yxinputFile = sc.textFile('bank.csv')
yxlines = yxinputFile.map(lambda line: line.split(';'))


(train, test) = yxlines.randomSplit([0.7, 0.3])

input_data_train = np.array(train.collect())
input_data_test = np.array(test.collect())

X_train = input_data_train[:,:-1]
y_train = input_data_train[:,16]

labelencode = LabelEncoder()
X_train[:,1] = labelencode.fit_transform(X_train[:,1])
X_train[:,2] = labelencode.fit_transform(X_train[:,2])
X_train[:,3] = labelencode.fit_transform(X_train[:,3])
X_train[:,4] = labelencode.fit_transform(X_train[:,4])
X_train[:,6] = labelencode.fit_transform(X_train[:,6])
X_train[:,7] = labelencode.fit_transform(X_train[:,7])
X_train[:,8] = labelencode.fit_transform(X_train[:,8])
X_train[:,15] = labelencode.fit_transform(X_train[:,15])
Y_train = labelencode.fit_transform(y_train)

X_train = np.delete(X_train, np.s_[9:11], 1)
X_train = np.array(X_train)
X_train = X_train.astype(np.float)
X_train = min_max_scaler.fit_transform(X_train)
Y_train = np.array(Y_train)

X_test = input_data_test[:,:-1]
y_test = input_data_test[:,16]



X_test[:,1] = labelencode.fit_transform(X_test[:,1])
X_test[:,2] = labelencode.fit_transform(X_test[:,2])
X_test[:,3] = labelencode.fit_transform(X_test[:,3])
X_test[:,4] = labelencode.fit_transform(X_test[:,4])
X_test[:,6] = labelencode.fit_transform(X_test[:,6])
X_test[:,7] = labelencode.fit_transform(X_test[:,7])
X_test[:,8] = labelencode.fit_transform(X_test[:,8])
X_test[:,15] = labelencode.fit_transform(X_test[:,15])
Y_test = labelencode.fit_transform(y_test)

X_test = np.delete(X_test, np.s_[9:11], 1)
X_test = np.array(X_test)
X_test = X_test.astype(np.float)
X_test = min_max_scaler.fit_transform(X_test)
Y_test = np.array(Y_test)





clf = LogisticRegression()
clf.fit(X_train,Y_train)
#print 'score Scikit learn: ', clf.score(X_test,Y_test)


##The sigmoid function adjusts the cost function hypotheses to adjust the algorithm proportionally for worse estimations
def Sigmoid(z):
	# print "in sigmoid"
	# print z
	#print(z)
	G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))
	# print G_of_Z
	return G_of_Z


def sigmoid(gamma):
  if gamma < 0:
    return float(1 - 1/(1 + math.exp(gamma)))
  else:
    return float(1/(1 + math.exp(-gamma)))

def Hypothesis(theta, x):
	z = 0
	theta_trans = np.transpose(theta)
	# print theta
	# print theta_trans
	z = np.dot(theta_trans,x)
	# print z, "vector"
	# z = 0
	# for i in range(len(theta)):
	# 	# print x[i]
	# 	# print theta[i]
	# 	z += x[i]*theta[i]
	# 	# print "called sigmoid of z"
	#print z
	return Sigmoid(z)


def Cost_Function(X,Y,theta,m):
	sumOfErrors = 0
	for i in range(m):
		xi = X[i]
		#break
		# print theta
		# print Y[i]
		hi = Hypothesis(theta,xi)
		
		if Y[i] == 1:
			#print "y[i]=1"+str(hi)
			error = Y[i] * math.log(hi)
		elif Y[i] == 0:
			#print "y[i]=0"+str(hi)
			error = (1-Y[i]) * math.log(1-hi)
		sumOfErrors += error
		#print "end of loop"
	const = -1/m
	J = const * sumOfErrors
	#print 'cost is ', J 
	return J

##This function creates the gradient component for each Theta value 
def Cost_Function_Derivative(X,Y,theta,j,m,alpha):
	sumErrors = 0
	
	for i in range(m):
		xi = X[i]
		#print xi
		xij = xi[j]
		#print xij
		hi = Hypothesis(theta,X[i])
		
		#print hi
		#print Y[i]
		error = (hi - Y[i])*xij
		#print error 
		sumErrors += error
	m = len(Y)
	constant = float(alpha)/float(m)
	J = constant * sumErrors
	return J

def Gradient_Descent(X,Y,theta,m,alpha):
	new_theta = []
	constant = alpha/m
	
	for j in range(len(theta)):
		CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)
		#print CFDerivative
		
		new_theta_value = theta[j] - CFDerivative
		new_theta.append(new_theta_value)
	return new_theta


def Logistic_Regression(X,Y,alpha,theta,num_iters):
	m = len(Y)
	#print X
	for x in range(num_iters):
		new_theta = Gradient_Descent(X,Y,theta,m,alpha)
		#break
		#print new_theta
		theta = new_theta
		if x % 100 == 0:
			#here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration
			Cost_Function(X,Y,theta,m)
			#print 'theta ', theta
			#print 'cost is ', Cost_Function(X,Y,theta,m)
	Declare_Winner(theta)

##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation
def Declare_Winner(theta):
	score = 0
	winner = ""
	scikit_score = clf.score(X_test,Y_test)
	length = len(X_test)
	for i in range(length):
		prediction = round(Hypothesis(X_test[i],theta))
		answer = Y_test[i]
		if prediction == answer:
			score += 1
	my_score = float(score) / float(length)
	# if my_score > scikit_score:
	# 	print 'You won!'
	# elif my_score == scikit_score:
	# 	print 'Its a tie!'
	# else:
	# 	print 'Scikit won.. :('
	print (my_score)
	#print 'Scikits score: ', scikit_score 


initial_theta = np.zeros(14)  #[0,0] 
alpha = 0.1
iterations = 1000
Logistic_Regression(X_train,Y_train,alpha,initial_theta,iterations)
sc.stop()