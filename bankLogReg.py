
import math
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder
import itertools
from pandas import DataFrame
from sklearn import preprocessing
from sklearn.linear_model import LogisticRegression
from sklearn.cross_validation import train_test_split
from numpy import loadtxt, where
from sklearn.metrics import confusion_matrix
import matplotlib.pyplot as plt
# scale larger positive and values to between -1,1 depending on the largest
min_max_scaler = preprocessing.MinMaxScaler(feature_range=(-1,1))


dataFrame = pd.read_csv('bank.csv',sep=';',index_col = None, header = None)
X = dataFrame.iloc[1:,:-1].values
Y = dataFrame.iloc[1:,16].values

labelencode = LabelEncoder()
X[:,1] = labelencode.fit_transform(X[:,1])
X[:,2] = labelencode.fit_transform(X[:,2])
X[:,3] = labelencode.fit_transform(X[:,3])
X[:,4] = labelencode.fit_transform(X[:,4])
X[:,6] = labelencode.fit_transform(X[:,6])
X[:,7] = labelencode.fit_transform(X[:,7])
X[:,8] = labelencode.fit_transform(X[:,8])
X[:,15] = labelencode.fit_transform(X[:,15])
Y = labelencode.fit_transform(Y)

X = np.delete(X, np.s_[9:11], 1)
X = np.array(X)
X = X.astype(np.float)
X = min_max_scaler.fit_transform(X)
Y = np.array(Y)


#fit remaining categorical columns and then try one hot encoding 



# creating testing and training set
X_train,X_test,Y_train,Y_test = train_test_split(X,Y,test_size=0.33, random_state = 42)

# train scikit learn model 
clf = LogisticRegression()
clf.fit(X_train,Y_train)
print 'score Scikit learn: ', clf.score(X_test,Y_test)


##The sigmoid function adjusts the cost function hypotheses to adjust the algorithm proportionally for worse estimations
def Sigmoid(z):
	# print "in sigmoid"
	# print z
	G_of_Z = float(1.0 / float((1.0 + math.exp(-1.0*z))))
	# print G_of_Z
	return G_of_Z




def Hypothesis(theta, x):
	z = 0
	for i in xrange(len(theta)):
		# print x[i]
		# print theta[i]
		z += x[i]*theta[i]
		# print "called sigmoid of z"
		# print z
	return Sigmoid(z)


def Cost_Function(X,Y,theta,m):
	sumOfErrors = 0
	for i in xrange(m):
		xi = X[i]
		# print theta
		# print Y[i]
		hi = Hypothesis(theta,xi)
		
		if Y[i] == 1:
			#print "y[i]=1"+str(hi)
			error = Y[i] * math.log(hi)
		elif Y[i] == 0:
			#print "y[i]=0"+str(hi)
			error = (1-Y[i]) * math.log(1-hi)
		sumOfErrors += error
		#print "end of loop"
	const = -1/m
	J = const * sumOfErrors
	print 'cost is ', J 
	return J

##This function creates the gradient component for each Theta value 
def Cost_Function_Derivative(X,Y,theta,j,m,alpha):
	sumErrors = 0
	for i in xrange(m):
		xi = X[i]
		xij = xi[j]
		hi = Hypothesis(theta,X[i])
		error = (hi - Y[i])*xij
		sumErrors += error
	m = len(Y)
	constant = float(alpha)/float(m)
	J = constant * sumErrors
	return J

def Gradient_Descent(X,Y,theta,m,alpha):
	new_theta = []
	constant = alpha/m
	for j in xrange(len(theta)):
		CFDerivative = Cost_Function_Derivative(X,Y,theta,j,m,alpha)
		new_theta_value = theta[j] - CFDerivative
		new_theta.append(new_theta_value)
	return new_theta


def Logistic_Regression(X,Y,alpha,theta,num_iters):
	m = len(Y)
	print X
	cost = []
	num_iterations = []
	for x in xrange(num_iters):
		new_theta = Gradient_Descent(X,Y,theta,m,alpha)
		#print new_theta
		theta = new_theta
		if x % 100 == 0:
			#here the cost function is used to present the final hypothesis of the model in the same form for each gradient-step iteration
			c = Cost_Function(X,Y,theta,m)
			cost.append(c)
			print 'theta ', theta
			print 'cost is ', c
			num_iterations.append(x)
	Declare_Winner(theta,cost,num_iterations)

##This method compares the accuracy of the model generated by the scikit library with the model generated by this implementation
def Declare_Winner(theta,cost,num_iterations):
	score = 0
	winner = ""
	scikit_score = clf.score(X_test,Y_test)
	plt.plot(num_iterations,cost)
	plt.show()
	Y_pred = []
	length = len(X_test)
	for i in xrange(length):
	 	prediction = round(Hypothesis(X_test[i],theta))
	 	Y_pred.append(prediction)
		answer = Y_test[i]
		if prediction == answer:
			score += 1
	my_score = float(score) / float(length)

	print 'Your score: ', my_score
	print 'Scikits score: ', scikit_score
	labels_pie = ['Correct Predictions', 'Incorrect Predictions']
	# print(score,incorrect_score)
	# values = [score, incorrect_score]
	# print("X_test",length)
	# colors = ['lightcoral', 'lightskyblue']
	# plt.pie(values, labels=labels_pie,colors=colors,autopct='%1.1f%%')
	# plt.axis('equal')
	# plt.show()
	Y_pred = np.matrix(Y_pred)
	# print(Y_test.shape, Y_pred.shape)
	# print(Y_pred)
	cnf_matrix = confusion_matrix(Y_test, Y_pred.T)	
	np.set_printoptions(precision=2)
	classes = ['Subscribed', 'Not Subscribed']
	plt.figure()
	plot_confusion_matrix(cnf_matrix, classes=classes,title='Confusion matrix')
	plt.show()

def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    # if normalize:
    #     cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
    #     print("Normalized confusion matrix")
    # else:
    #     print('Confusion matrix, without normalization')

    # print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

initial_theta = np.zeros(14)  #[0,0] 
alpha = 0.1
iterations = 1000
Logistic_Regression(X,Y,alpha,initial_theta,iterations)



